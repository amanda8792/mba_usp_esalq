{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e2e0dc-dd97-40a1-8bbd-f42fd462ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.cm import rainbow\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import skew\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c71161-8e7c-4d4a-8366-2ab11b7e3d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17979691-9e96-40c6-803a-e82b3eb75df8",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4764e20e-a888-4e2f-a622-3c7fad4cac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('application_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c897c6-a2fd-4a2d-8e07-f7f1120e47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 01 - Tratamento e limpeza dos dados nulos\n",
    "\n",
    "docs_colunas = ['FLAG_DOCUMENT_' + str(i) for i in range(2, 22)]\n",
    "df['NUM_DOCS_APRESENTADOS'] = df[docs_colunas].sum(axis=1)\n",
    "flag_docs_colunas = [col for col in df.columns if col.startswith('FLAG_DOCUMENT_')]\n",
    "df_exclusao_cols = df.drop(columns=flag_docs_colunas)\n",
    "df = df_exclusao_cols\n",
    "\n",
    "# Exclusão de colunas com valores nulos\n",
    "\n",
    "def excluir_colunas_nulas(df, percentual):\n",
    "    dados_faltantes = df.isnull().mean() * 100\n",
    "    col_para_apagar = dados_faltantes[dados_faltantes > percentual].index.tolist()\n",
    "    return col_para_apagar\n",
    "\n",
    "percentual = 0\n",
    "col_para_apagar = excluir_colunas_nulas(df, percentual)\n",
    "\n",
    "# Exclui as colunas identificadas\n",
    "df_exclusao_colunas = df.drop(columns=col_para_apagar)\n",
    "df = df_exclusao_colunas\n",
    "\n",
    "# Exclusão de colunas de categoria única\n",
    "\n",
    "columns_to_drop = ['FLAG_MOBIL',\n",
    "                   'FLAG_EMP_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_EMAIL','REG_REGION_NOT_WORK_REGION',\n",
    "                   'REG_REGION_NOT_LIVE_REGION','REG_CITY_NOT_WORK_CITY','REG_CITY_NOT_LIVE_CITY']\n",
    "df = df.drop(columns_to_drop,axis=1)\n",
    "\n",
    "# Exclusão dos valores 'XNA' (essencialmente nulos) em CODE_GENDER\n",
    "\n",
    "df = df[df['CODE_GENDER'] != 'XNA']\n",
    "\n",
    "# Conversao de algumas colunas para anos\n",
    "\n",
    "# Colunas que terão o sinal removido (valores absolutos)\n",
    "colunas_para_remover_sinal = ['DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH']\n",
    "df[colunas_para_remover_sinal] = df[colunas_para_remover_sinal].abs()\n",
    "\n",
    "# Engenharia de atributos\n",
    "df['IDADE'] = df['DAYS_BIRTH'] // 365\n",
    "df['ANOS_TRABALHADOS'] = df['DAYS_EMPLOYED'] // 365\n",
    "df['ANOS_REGISTRO'] = df['DAYS_REGISTRATION'] // 365\n",
    "df['ANOS_PUBLICACAO_ID'] = df['DAYS_ID_PUBLISH'] // 365\n",
    "\n",
    "# Remover colunas originais\n",
    "df = df.drop(colunas_para_remover_sinal, axis=1)\n",
    "\n",
    "#  Devido a uma alta correlação, decidi remover Region_Rating_Client_With_City\n",
    "\n",
    "column_to_drop = ['REGION_RATING_CLIENT_W_CITY']\n",
    "df1 = df.drop(column_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc228692-1c2a-4b63-9684-9dd9e9089dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 307507 entries, 0 to 307510\n",
      "Data columns (total 27 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   SK_ID_CURR                   307507 non-null  int64  \n",
      " 1   TARGET                       307507 non-null  int64  \n",
      " 2   NAME_CONTRACT_TYPE           307507 non-null  object \n",
      " 3   CODE_GENDER                  307507 non-null  object \n",
      " 4   FLAG_OWN_CAR                 307507 non-null  object \n",
      " 5   FLAG_OWN_REALTY              307507 non-null  object \n",
      " 6   CNT_CHILDREN                 307507 non-null  int64  \n",
      " 7   AMT_INCOME_TOTAL             307507 non-null  float64\n",
      " 8   AMT_CREDIT                   307507 non-null  float64\n",
      " 9   NAME_INCOME_TYPE             307507 non-null  object \n",
      " 10  NAME_EDUCATION_TYPE          307507 non-null  object \n",
      " 11  NAME_FAMILY_STATUS           307507 non-null  object \n",
      " 12  NAME_HOUSING_TYPE            307507 non-null  object \n",
      " 13  REGION_POPULATION_RELATIVE   307507 non-null  float64\n",
      " 14  FLAG_WORK_PHONE              307507 non-null  int64  \n",
      " 15  FLAG_PHONE                   307507 non-null  int64  \n",
      " 16  REGION_RATING_CLIENT         307507 non-null  int64  \n",
      " 17  WEEKDAY_APPR_PROCESS_START   307507 non-null  object \n",
      " 18  HOUR_APPR_PROCESS_START      307507 non-null  int64  \n",
      " 19  LIVE_REGION_NOT_WORK_REGION  307507 non-null  int64  \n",
      " 20  LIVE_CITY_NOT_WORK_CITY      307507 non-null  int64  \n",
      " 21  ORGANIZATION_TYPE            307507 non-null  object \n",
      " 22  NUM_DOCS_APRESENTADOS        307507 non-null  int64  \n",
      " 23  IDADE                        307507 non-null  int64  \n",
      " 24  ANOS_TRABALHADOS             307507 non-null  int64  \n",
      " 25  ANOS_REGISTRO                307507 non-null  float64\n",
      " 26  ANOS_PUBLICACAO_ID           307507 non-null  int64  \n",
      "dtypes: float64(4), int64(13), object(10)\n",
      "memory usage: 65.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf751b-9bde-451d-a3ab-19a43059e731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d3d0b8e-dd98-4a5b-894f-cedd41d77a4e",
   "metadata": {},
   "source": [
    "## Árvore 1 - Base após EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ac5a89-8171-4324-b067-0779b77cbe6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 307507 entries, 0 to 307510\n",
      "Data columns (total 17 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   SK_ID_CURR                   307507 non-null  int64  \n",
      " 1   TARGET                       307507 non-null  int64  \n",
      " 2   CNT_CHILDREN                 307507 non-null  int64  \n",
      " 3   AMT_INCOME_TOTAL             307507 non-null  float64\n",
      " 4   AMT_CREDIT                   307507 non-null  float64\n",
      " 5   REGION_POPULATION_RELATIVE   307507 non-null  float64\n",
      " 6   FLAG_WORK_PHONE              307507 non-null  int64  \n",
      " 7   FLAG_PHONE                   307507 non-null  int64  \n",
      " 8   REGION_RATING_CLIENT         307507 non-null  int64  \n",
      " 9   HOUR_APPR_PROCESS_START      307507 non-null  int64  \n",
      " 10  LIVE_REGION_NOT_WORK_REGION  307507 non-null  int64  \n",
      " 11  LIVE_CITY_NOT_WORK_CITY      307507 non-null  int64  \n",
      " 12  NUM_DOCS_APRESENTADOS        307507 non-null  int64  \n",
      " 13  IDADE                        307507 non-null  int64  \n",
      " 14  ANOS_TRABALHADOS             307507 non-null  int64  \n",
      " 15  ANOS_REGISTRO                307507 non-null  float64\n",
      " 16  ANOS_PUBLICACAO_ID           307507 non-null  int64  \n",
      "dtypes: float64(4), int64(13)\n",
      "memory usage: 42.2 MB\n",
      "base para árvore:  None\n"
     ]
    }
   ],
   "source": [
    "# Selecionar colunas numéricas (int64 e float64)\n",
    "colunas_numericas = df1.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "df_arvore_1 = df1[colunas_numericas]\n",
    "print('base para árvore: ', df_arvore_1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5e0e8c1-7788-4ad5-ae14-511a3e39e4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.8403\n",
      "Precisão: 0.1076\n",
      "Recall: 0.1310\n",
      "F1-Score: 0.1181\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[51021  5460]\n",
      " [ 4363   658]]\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91     56481\n",
      "           1       0.11      0.13      0.12      5021\n",
      "\n",
      "    accuracy                           0.84     61502\n",
      "   macro avg       0.51      0.52      0.52     61502\n",
      "weighted avg       0.85      0.84      0.85     61502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Passo 1: Preparação dos Dados\n",
    "# Passo 2: Treinamento da Árvore de Decisão\n",
    "# Passo 3: Avaliação do Modelo\n",
    "\n",
    "# Definir as features e o target\n",
    "X = df_arvore_1.drop(columns=['TARGET'])  # Todas as variáveis menos o target\n",
    "y = df_arvore_1['TARGET']  # O target\n",
    "\n",
    "# Dividir os dados em treino e teste (80% treino, 20% teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar o modelo de árvore de decisão\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões nos dados de teste\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# Avaliar o desempenho do modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f'Acurácia: {accuracy:.4f}')\n",
    "print(f'Precisão: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')\n",
    "print('\\nMatriz de Confusão:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\nRelatório de Classificação:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df83e2ce-43ac-4891-aa79-f5e5710bf567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Melhores parâmetros: {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Acurácia: 0.5877\n",
      "Precisão: 0.1129\n",
      "Recall: 0.5909\n",
      "F1-Score: 0.1896\n"
     ]
    }
   ],
   "source": [
    "# Treinamento da árvore com class_weight balanceado\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parâmetros a serem ajustados\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Modelo de árvore\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Grid Search com validação cruzada\n",
    "grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=5, scoring='f1', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Treinamento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Avaliação\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"Melhores parâmetros: {grid_search.best_params_}\")\n",
    "print(f'Acurácia: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print(f'Precisão: {precision_score(y_test, y_pred):.4f}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred):.4f}')\n",
    "print(f'F1-Score: {f1_score(y_test, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec4759-f879-43c0-b253-253240921842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed2d616d-855a-4f74-9009-147818368637",
   "metadata": {},
   "source": [
    "Ao analisar os resultados das duas abordagens para a detecção de fraudes:\n",
    "\n",
    "### Primeira Árvore (sem `class_weight` balanceado):\n",
    "- **Acurácia**: 0.8403\n",
    "- **Precisão**: 0.1076\n",
    "- **Recall**: 0.1310\n",
    "- **F1-Score**: 0.1181\n",
    "\n",
    "#### Interpretação:\n",
    "- Alta acurácia (0.84), mas isso é esperado, já que a classe \"não fraude\" é majoritária, e o modelo está classificando corretamente a maioria das instâncias \"não fraude\".\n",
    "- **Baixa precisão e recall para fraudes** (classe 1): O modelo tem dificuldade em identificar fraudes, resultando em muitos falsos negativos e falsos positivos.\n",
    "- **F1-Score baixo** (0.1181): O modelo não está conseguindo equilibrar bem a precisão e o recall, o que o torna menos eficiente na detecção de fraudes.\n",
    "\n",
    "### Segunda Árvore (com `class_weight` balanceado):\n",
    "- **Acurácia**: 0.5877\n",
    "- **Precisão**: 0.1129\n",
    "- **Recall**: 0.5909\n",
    "- **F1-Score**: 0.1896\n",
    "\n",
    "#### Interpretação:\n",
    "- A **acurácia diminuiu** significativamente (0.5877), mas isso era esperado, pois o foco agora é melhorar a detecção da classe minoritária (fraudes), e o modelo deixa de priorizar a classe \"não fraude\".\n",
    "- **Recall** para fraudes **melhorou muito** (0.5909): O modelo agora está capturando muito mais fraudes, o que é importante, pois é preferível detectar mais casos de fraude, mesmo que isso implique em mais falsos positivos.\n",
    "- **Precisão** continua baixa (0.1129), o que indica que, embora o modelo esteja capturando mais fraudes, ainda há muitos falsos positivos.\n",
    "- **F1-Score aumentou** (0.1896), o que mostra uma melhoria no equilíbrio entre precisão e recall, mas ainda é um valor relativamente baixo para fraudes.\n",
    "\n",
    "### Análise Geral:\n",
    "- **Primeira Árvore**: Tem uma alta acurácia, mas um desempenho muito ruim na detecção de fraudes, com baixa precisão e recall.\n",
    "- **Segunda Árvore (com `class_weight` balanceado)**: Focou melhor na detecção de fraudes, com um recall significativamente maior, o que é crucial para problemas de fraudes. No entanto, isso custou uma queda acentuada na acurácia geral e a precisão ainda é baixa.\n",
    "\n",
    "### Conclusão:\n",
    "A segunda abordagem, com `class_weight='balanced'`, apesar de ter uma acurácia geral mais baixa, é **mais adequada para a detecção de fraudes**, pois aumenta consideravelmente o recall, permitindo capturar mais casos de fraude. No entanto, há espaço para melhorias, principalmente em termos de precisão, para reduzir falsos positivos. Ajustes adicionais nos hiperparâmetros ou o uso de outras técnicas, como ensemble methods (e.g., Random Forest ou XGBoost), podem melhorar o equilíbrio entre precisão e recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a96126-fe80-42ed-81f4-5b18094e905d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463fe976-44f5-44ce-8c0a-cf5ec439bc30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb195f-a6f2-4e07-bac8-3e8e72573b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ba553af-4c91-4ab0-ba52-7273ffa87b9a",
   "metadata": {},
   "source": [
    "## Árvore 2 - Base completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c3d94de-398c-4a95-980d-9d2d94634e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados\n",
    "df2 = pd.read_csv('application_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a891529-3d58-4a58-b3b8-319d471c062d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 307511 entries, 0 to 307510\n",
      "Data columns (total 86 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   TARGET                        307511 non-null  int64  \n",
      " 1   CNT_CHILDREN                  307511 non-null  int64  \n",
      " 2   AMT_CREDIT                    307511 non-null  float64\n",
      " 3   AMT_ANNUITY                   307499 non-null  float64\n",
      " 4   AMT_GOODS_PRICE               307233 non-null  float64\n",
      " 5   REGION_POPULATION_RELATIVE    307511 non-null  float64\n",
      " 6   DAYS_BIRTH                    307511 non-null  int64  \n",
      " 7   DAYS_EMPLOYED                 307511 non-null  int64  \n",
      " 8   DAYS_REGISTRATION             307511 non-null  float64\n",
      " 9   DAYS_ID_PUBLISH               307511 non-null  int64  \n",
      " 10  OWN_CAR_AGE                   104582 non-null  float64\n",
      " 11  FLAG_EMP_PHONE                307511 non-null  int64  \n",
      " 12  FLAG_WORK_PHONE               307511 non-null  int64  \n",
      " 13  FLAG_PHONE                    307511 non-null  int64  \n",
      " 14  CNT_FAM_MEMBERS               307509 non-null  float64\n",
      " 15  REGION_RATING_CLIENT          307511 non-null  int64  \n",
      " 16  REGION_RATING_CLIENT_W_CITY   307511 non-null  int64  \n",
      " 17  HOUR_APPR_PROCESS_START       307511 non-null  int64  \n",
      " 18  REG_REGION_NOT_LIVE_REGION    307511 non-null  int64  \n",
      " 19  REG_REGION_NOT_WORK_REGION    307511 non-null  int64  \n",
      " 20  REG_CITY_NOT_LIVE_CITY        307511 non-null  int64  \n",
      " 21  REG_CITY_NOT_WORK_CITY        307511 non-null  int64  \n",
      " 22  LIVE_CITY_NOT_WORK_CITY       307511 non-null  int64  \n",
      " 23  EXT_SOURCE_1                  134133 non-null  float64\n",
      " 24  EXT_SOURCE_2                  306851 non-null  float64\n",
      " 25  EXT_SOURCE_3                  246546 non-null  float64\n",
      " 26  APARTMENTS_AVG                151450 non-null  float64\n",
      " 27  BASEMENTAREA_AVG              127568 non-null  float64\n",
      " 28  YEARS_BEGINEXPLUATATION_AVG   157504 non-null  float64\n",
      " 29  YEARS_BUILD_AVG               103023 non-null  float64\n",
      " 30  COMMONAREA_AVG                92646 non-null   float64\n",
      " 31  ELEVATORS_AVG                 143620 non-null  float64\n",
      " 32  ENTRANCES_AVG                 152683 non-null  float64\n",
      " 33  FLOORSMAX_AVG                 154491 non-null  float64\n",
      " 34  FLOORSMIN_AVG                 98869 non-null   float64\n",
      " 35  LANDAREA_AVG                  124921 non-null  float64\n",
      " 36  LIVINGAPARTMENTS_AVG          97312 non-null   float64\n",
      " 37  LIVINGAREA_AVG                153161 non-null  float64\n",
      " 38  NONLIVINGAREA_AVG             137829 non-null  float64\n",
      " 39  APARTMENTS_MODE               151450 non-null  float64\n",
      " 40  BASEMENTAREA_MODE             127568 non-null  float64\n",
      " 41  YEARS_BEGINEXPLUATATION_MODE  157504 non-null  float64\n",
      " 42  YEARS_BUILD_MODE              103023 non-null  float64\n",
      " 43  COMMONAREA_MODE               92646 non-null   float64\n",
      " 44  ELEVATORS_MODE                143620 non-null  float64\n",
      " 45  ENTRANCES_MODE                152683 non-null  float64\n",
      " 46  FLOORSMAX_MODE                154491 non-null  float64\n",
      " 47  FLOORSMIN_MODE                98869 non-null   float64\n",
      " 48  LANDAREA_MODE                 124921 non-null  float64\n",
      " 49  LIVINGAPARTMENTS_MODE         97312 non-null   float64\n",
      " 50  LIVINGAREA_MODE               153161 non-null  float64\n",
      " 51  NONLIVINGAREA_MODE            137829 non-null  float64\n",
      " 52  APARTMENTS_MEDI               151450 non-null  float64\n",
      " 53  BASEMENTAREA_MEDI             127568 non-null  float64\n",
      " 54  YEARS_BEGINEXPLUATATION_MEDI  157504 non-null  float64\n",
      " 55  YEARS_BUILD_MEDI              103023 non-null  float64\n",
      " 56  COMMONAREA_MEDI               92646 non-null   float64\n",
      " 57  ELEVATORS_MEDI                143620 non-null  float64\n",
      " 58  ENTRANCES_MEDI                152683 non-null  float64\n",
      " 59  FLOORSMAX_MEDI                154491 non-null  float64\n",
      " 60  FLOORSMIN_MEDI                98869 non-null   float64\n",
      " 61  LANDAREA_MEDI                 124921 non-null  float64\n",
      " 62  LIVINGAPARTMENTS_MEDI         97312 non-null   float64\n",
      " 63  LIVINGAREA_MEDI               153161 non-null  float64\n",
      " 64  NONLIVINGAREA_MEDI            137829 non-null  float64\n",
      " 65  TOTALAREA_MODE                159080 non-null  float64\n",
      " 66  OBS_30_CNT_SOCIAL_CIRCLE      306490 non-null  float64\n",
      " 67  DEF_30_CNT_SOCIAL_CIRCLE      306490 non-null  float64\n",
      " 68  OBS_60_CNT_SOCIAL_CIRCLE      306490 non-null  float64\n",
      " 69  DEF_60_CNT_SOCIAL_CIRCLE      306490 non-null  float64\n",
      " 70  DAYS_LAST_PHONE_CHANGE        307510 non-null  float64\n",
      " 71  FLAG_DOCUMENT_3               307511 non-null  int64  \n",
      " 72  FLAG_DOCUMENT_4               307511 non-null  int64  \n",
      " 73  FLAG_DOCUMENT_6               307511 non-null  int64  \n",
      " 74  FLAG_DOCUMENT_8               307511 non-null  int64  \n",
      " 75  FLAG_DOCUMENT_9               307511 non-null  int64  \n",
      " 76  FLAG_DOCUMENT_10              307511 non-null  int64  \n",
      " 77  FLAG_DOCUMENT_11              307511 non-null  int64  \n",
      " 78  FLAG_DOCUMENT_13              307511 non-null  int64  \n",
      " 79  FLAG_DOCUMENT_14              307511 non-null  int64  \n",
      " 80  FLAG_DOCUMENT_15              307511 non-null  int64  \n",
      " 81  FLAG_DOCUMENT_16              307511 non-null  int64  \n",
      " 82  FLAG_DOCUMENT_17              307511 non-null  int64  \n",
      " 83  FLAG_DOCUMENT_18              307511 non-null  int64  \n",
      " 84  AMT_REQ_CREDIT_BUREAU_MON     265992 non-null  float64\n",
      " 85  AMT_REQ_CREDIT_BUREAU_YEAR    265992 non-null  float64\n",
      "dtypes: float64(57), int64(29)\n",
      "memory usage: 201.8 MB\n",
      "base árvore 2:  None\n"
     ]
    }
   ],
   "source": [
    "variables_to_test = df2.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "alpha = 0.05\n",
    "significant_variables = []\n",
    "\n",
    "for variable in variables_to_test:\n",
    "    # Extrair dados para os grupos de inadimplência (TARGET = 1) e não inadimplência (TARGET = 0)\n",
    "    data_default = df2[df2['TARGET'] == 1][variable].dropna()  # Remover valores nulos\n",
    "    data_no_default = df2[df2['TARGET'] == 0][variable].dropna()  # Remover valores nulos\n",
    "\n",
    "    # Realizar o teste t para duas amostras\n",
    "    t_statistic, p_value = stats.ttest_ind(data_default, data_no_default, equal_var=False)\n",
    "\n",
    "    # Verificar significância\n",
    "    if p_value < alpha:\n",
    "        significant_variables.append(variable)\n",
    "\n",
    "# Criar um novo DataFrame apenas com as variáveis significativas\n",
    "df20 = df2[significant_variables]\n",
    "\n",
    "def identify_columns_to_drop(df, percentage):\n",
    "    missing_percentage = df.isnull().mean() * 100\n",
    "    columns_to_drop = missing_percentage[missing_percentage > percentage].index.tolist()\n",
    "    return columns_to_drop\n",
    "\n",
    "percentage = 0\n",
    "columns_to_drop = identify_columns_to_drop(df20, percentage)\n",
    "\n",
    "# Exclui as colunas identificadas\n",
    "df_dropped = df20.drop(columns=columns_to_drop)\n",
    "df_arvore_2 = df_dropped\n",
    "\n",
    "#print('base árvore 2: ', df_arvore_2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8c1bbf3-51d9-418c-884f-f54243abbca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.8423\n",
      "Precisão: 0.1017\n",
      "Recall: 0.1224\n",
      "F1-Score: 0.1111\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[51200  5354]\n",
      " [ 4343   606]]\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91     56554\n",
      "           1       0.10      0.12      0.11      4949\n",
      "\n",
      "    accuracy                           0.84     61503\n",
      "   macro avg       0.51      0.51      0.51     61503\n",
      "weighted avg       0.86      0.84      0.85     61503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Passo 1: Preparação dos Dados\n",
    "# Passo 2: Treinamento da Árvore de Decisão\n",
    "# Passo 3: Avaliação do Modelo\n",
    "\n",
    "X = df_arvore_2.drop(columns=['TARGET'])  # Todas as variáveis menos o target\n",
    "y = df_arvore_2['TARGET']  # O target\n",
    "\n",
    "# Dividir os dados em treino e teste (80% treino, 20% teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar o modelo de árvore de decisão\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões nos dados de teste\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# Avaliar o desempenho do modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f'Acurácia: {accuracy:.4f}')\n",
    "print(f'Precisão: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')\n",
    "print('\\nMatriz de Confusão:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\nRelatório de Classificação:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f4f4c43-4fe3-4ce4-b89b-93af9219309d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Melhores parâmetros: {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Acurácia: 0.5934\n",
      "Precisão: 0.1125\n",
      "Recall: 0.5886\n",
      "F1-Score: 0.1890\n"
     ]
    }
   ],
   "source": [
    "# Treinamento da árvore com class_weight balanceado\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parâmetros a serem ajustados\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Modelo de árvore\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Grid Search com validação cruzada\n",
    "grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=5, scoring='f1', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Treinamento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Avaliação\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"Melhores parâmetros: {grid_search.best_params_}\")\n",
    "print(f'Acurácia: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print(f'Precisão: {precision_score(y_test, y_pred):.4f}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred):.4f}')\n",
    "print(f'F1-Score: {f1_score(y_test, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a913f7-52ad-4a28-9bb4-96dcb7ecfd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eccd4f76-3292-442a-92d4-ddbdbaf71a3e",
   "metadata": {},
   "source": [
    "Ao comparar os dois cenários, podemos avaliar o desempenho do modelo de detecção de fraudes:\n",
    "\n",
    "1. **Árvore original**:\n",
    "   - Acurácia: 0.8423\n",
    "   - Precisão: 0.1017\n",
    "   - Recall: 0.1224\n",
    "   - F1-Score: 0.1111\n",
    "\n",
    "   A árvore original tem uma boa acurácia (84%), mas a precisão e o recall para a classe fraudulenta (classe 1) são muito baixos. Isso indica que o modelo está identificando poucas fraudes e, quando identifica, muitos dos casos são falsos positivos. O F1-Score de 0.1111 reflete o equilíbrio pobre entre precisão e recall.\n",
    "\n",
    "2. **Árvore com class_weight='balanced'**:\n",
    "   - Acurácia: 0.5934\n",
    "   - Precisão: 0.1125\n",
    "   - Recall: 0.5886\n",
    "   - F1-Score: 0.1890\n",
    "\n",
    "   Ao ajustar o modelo para `class_weight='balanced'`, houve uma redução significativa na acurácia geral (59%), o que era esperado, já que o modelo está sendo ajustado para equilibrar as classes. O recall para a classe 1 (fraudes) melhorou muito, subindo para 58.9%, o que significa que o modelo está identificando mais fraudes. No entanto, a precisão continua baixa (11.25%), indicando que muitos falsos positivos ainda estão sendo gerados. O F1-Score de 0.1890 é melhor que o anterior, mostrando que o modelo está conseguindo melhorar o equilíbrio entre precisão e recall.\n",
    "\n",
    "### Conclusão:\n",
    "Embora a acurácia tenha diminuído na segunda abordagem, o aumento no recall é crucial para detecção de fraudes, pois identificar o maior número possível de fraudes (mesmo com alguns falsos positivos) é muitas vezes mais importante do que manter uma acurácia alta. O modelo com `class_weight='balanced'` parece ser mais adequado para detecção de fraudes, especialmente se o objetivo for minimizar fraudes não detectadas. Para melhorar ainda mais, pode ser interessante explorar outras técnicas de ajuste de hiperparâmetros ou algoritmos, como Random Forest ou XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e820e-2cfb-4480-abff-9745776e4c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd2ebb1-cd77-4c29-9b7c-29f42c9cf6f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03749329-7828-4cdd-854b-dc3db2f2bf4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb19a3-1f79-45ff-9f23-4caa673897e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8042e580-d9f8-47a3-96d1-5b8556c5e813",
   "metadata": {},
   "source": [
    "## Árvore 3 - Tentativa melhoria (a melhor até entao rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b33a97c3-9ef4-4c84-9411-6497243821a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados\n",
    "df3 = pd.read_csv('application_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5eb8c7c6-6dd4-4b71-b95d-0d578dc6d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criação da coluna NUM_DOCUMENTS_PROVIDED\n",
    "\n",
    "document_columns = ['FLAG_DOCUMENT_' + str(i) for i in range(2, 22)]\n",
    "df3['NUM_DOCUMENTS_PROVIDED'] = df3[document_columns].sum(axis=1)\n",
    "\n",
    "# exclusao das colunas FLAG_DOCUMENT_X\n",
    "column = ['FLAG_DOCUMENT_2',\n",
    "       'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5',\n",
    "       'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8',\n",
    "       'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11',\n",
    "       'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14',\n",
    "       'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17',\n",
    "       'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20',\n",
    "       'FLAG_DOCUMENT_21']\n",
    "df3 = df3.drop(column, axis=1)\n",
    "\n",
    "# exclusao de colunas com mais de 19% de dados nulos\n",
    "columns_to_drop = ['OWN_CAR_AGE', 'EXT_SOURCE_1', 'EXT_SOURCE_3', 'EXT_SOURCE_3','APARTMENTS_AVG', 'BASEMENTAREA_AVG', \n",
    "                   'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'ELEVATORS_AVG',\n",
    "                   'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG',\n",
    "                   'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG', 'APARTMENTS_MODE',\n",
    "                   'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE',\n",
    "                   'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE',\n",
    "                   'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE',\n",
    "                   'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI',\n",
    "                   'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI',\n",
    "                   'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI',\n",
    "                   'NONLIVINGAREA_MEDI', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'TOTALAREA_MODE',\n",
    "                   'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE']\n",
    "df3 = df3.drop(columns=columns_to_drop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86dc89e9-9eba-41c3-b829-eb3e8a56a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamento dos valores nulos \n",
    "numerical_columns_low_missing = ['AMT_ANNUITY', 'AMT_GOODS_PRICE', 'CNT_FAM_MEMBERS', 'EXT_SOURCE_2',\n",
    "                                  'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE',\n",
    "                                  'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'DAYS_LAST_PHONE_CHANGE']\n",
    "df3[numerical_columns_low_missing] = df3[numerical_columns_low_missing].fillna(df3[numerical_columns_low_missing].median())\n",
    "\n",
    "df3['OCCUPATION_TYPE'] = df3['OCCUPATION_TYPE'].fillna('unknown')\n",
    "df3['NAME_TYPE_SUITE'] = df3['NAME_TYPE_SUITE'].fillna(df3['NAME_TYPE_SUITE'].mode()[0])\n",
    "\n",
    "rows_to_drop = df3[(df3['TARGET'] == 0) & df3[['AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR']].isnull().any(axis=1)].index\n",
    "df3 = df3.drop(rows_to_drop)\n",
    "\n",
    "df3['AMT_REQ_CREDIT_BUREAU_YEAR'] = pd.to_numeric(df3['AMT_REQ_CREDIT_BUREAU_YEAR'], errors='coerce')\n",
    "\n",
    "bin_edges = [0, 5, 10, 20]  # Adjust the bin edges as needed\n",
    "bin_labels = ['Low from 0 to 5', 'Medium from 6 to 10', 'High +11']\n",
    "\n",
    "df3['AMT_REQ_CREDIT_BUREAU_YEAR_Binned'] = pd.cut(df3['AMT_REQ_CREDIT_BUREAU_YEAR'], bins=bin_edges, labels=bin_labels, right=False)\n",
    "\n",
    "unknown_category = 'unknown'\n",
    "df3['AMT_REQ_CREDIT_BUREAU_YEAR_Binned'] = df3['AMT_REQ_CREDIT_BUREAU_YEAR_Binned'].cat.add_categories([unknown_category])\n",
    "df3['AMT_REQ_CREDIT_BUREAU_YEAR_Binned'] = df3['AMT_REQ_CREDIT_BUREAU_YEAR_Binned'].fillna(unknown_category)\n",
    "bin_counts = df3['AMT_REQ_CREDIT_BUREAU_YEAR_Binned'].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba5850c2-cdba-43f4-a3b6-8d6e1aea066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column to numeric, coercing errors to NaN\n",
    "df3['AMT_REQ_CREDIT_BUREAU_QRT'] = pd.to_numeric(df3['AMT_REQ_CREDIT_BUREAU_QRT'], errors='coerce')\n",
    "\n",
    "# Define bin edges and labels\n",
    "bin_edges = [0, 5, float('inf')]  # Adjust the bin edges as needed\n",
    "bin_labels = ['Low from 0 to 5', 'High above 5']\n",
    "\n",
    "# Create a new column with bins\n",
    "df3['AMT_REQ_CREDIT_BUREAU_QRT_Binned'] = pd.cut(df3['AMT_REQ_CREDIT_BUREAU_QRT'], bins=bin_edges, labels=bin_labels, right=False)\n",
    "\n",
    "unknown_category = 'unknown'\n",
    "df3['AMT_REQ_CREDIT_BUREAU_QRT_Binned'] = df3['AMT_REQ_CREDIT_BUREAU_QRT_Binned'].cat.add_categories([unknown_category])\n",
    "df3['AMT_REQ_CREDIT_BUREAU_QRT_Binned'] = df3['AMT_REQ_CREDIT_BUREAU_QRT_Binned'].fillna(unknown_category)\n",
    "\n",
    "# Display the counts in each bin\n",
    "bin_counts = df3['AMT_REQ_CREDIT_BUREAU_QRT_Binned'].value_counts().sort_index()\n",
    "\n",
    "# Convert the column to numeric, coercing errors to NaN\n",
    "df3['AMT_REQ_CREDIT_BUREAU_MON'] = pd.to_numeric(df3['AMT_REQ_CREDIT_BUREAU_MON'], errors='coerce')\n",
    "\n",
    "# Define bin edges and labels\n",
    "bin_edges = [0, 5, 10, float('inf')]  # Adjust the bin edges as needed\n",
    "bin_labels = ['Low from 0 to 5', 'Medium from 6 to 10', 'High +10']\n",
    "\n",
    "# Create a new column with bins\n",
    "df3['AMT_REQ_CREDIT_BUREAU_MON_Binned'] = pd.cut(df3['AMT_REQ_CREDIT_BUREAU_MON'], bins=bin_edges, labels=bin_labels, right=False)\n",
    "\n",
    "unknown_category = 'unknown'\n",
    "df3['AMT_REQ_CREDIT_BUREAU_MON_Binned'] = df3['AMT_REQ_CREDIT_BUREAU_MON_Binned'].cat.add_categories([unknown_category])\n",
    "df3['AMT_REQ_CREDIT_BUREAU_MON_Binned'] = df3['AMT_REQ_CREDIT_BUREAU_MON_Binned'].fillna(unknown_category)\n",
    "\n",
    "# Display the counts in each bin\n",
    "bin_counts = df3['AMT_REQ_CREDIT_BUREAU_MON_Binned'].value_counts().sort_index()\n",
    "\n",
    "columns_to_process = ['AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_HOUR']\n",
    "df3[columns_to_process] = df3[columns_to_process].apply(pd.to_numeric, errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f24d1f71-93e6-4506-aed6-5d80ce4d7286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bin edges and labels\n",
    "bin_edges = [0, 2, float('inf')]  # Adjust the bin edges as needed\n",
    "bin_labels = ['Low from 0 to 2', 'High above 2']\n",
    "\n",
    "# Iterate over columns\n",
    "for column in columns_to_process:\n",
    "    # Create a new column with bins\n",
    "    bin_column_name = f'{column}_Binned'\n",
    "    df3[bin_column_name] = pd.cut(df3[column], bins=bin_edges, labels=bin_labels, right=False)\n",
    "\n",
    "    # Add an 'unknown' category\n",
    "    unknown_category = 'unknown'\n",
    "    df3[bin_column_name] = df3[bin_column_name].cat.add_categories([unknown_category])\n",
    "\n",
    "    # Fill missing values with 'unknown'\n",
    "    df3[bin_column_name] = df3[bin_column_name].fillna(unknown_category)\n",
    "\n",
    "    # Display the counts in each bin\n",
    "    bin_counts = df3[bin_column_name].value_counts().sort_index()\n",
    "#    print(f\"Counts for {bin_column_name}:\")\n",
    "#    print(bin_counts)\n",
    "\n",
    "column = ['AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON',\n",
    "         'AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR']\n",
    "df3 = df3.drop(column, axis=1)\n",
    "\n",
    "categorical_columns = ['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'SK_ID_CURR','FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL', 'NUM_DOCUMENTS_PROVIDED',\n",
    "                       'REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY','REG_CITY_NOT_LIVE_CITY','REG_CITY_NOT_WORK_CITY',\n",
    "                      'REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION','LIVE_REGION_NOT_WORK_REGION','LIVE_CITY_NOT_WORK_CITY']\n",
    "\n",
    "df3[categorical_columns] = df3[categorical_columns].astype('category')\n",
    "\n",
    "df3['SK_ID_CURR'] = df3['SK_ID_CURR'].astype('object')\n",
    "\n",
    "columns_to_remove_sign = ['DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH','DAYS_LAST_PHONE_CHANGE']\n",
    "df3[columns_to_remove_sign] = df3[columns_to_remove_sign].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c129f8e-5b60-4d24-a541-db6f8ae60bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBS_30_CNT_SOCIAL_CIRCLE_Binned:\n",
      "OBS_30_CNT_SOCIAL_CIRCLE_Binned\n",
      "0-5     244096\n",
      "6-10     22518\n",
      "11+       3670\n",
      "Name: count, dtype: int64\n",
      "\n",
      "OBS_60_CNT_SOCIAL_CIRCLE_Binned:\n",
      "OBS_60_CNT_SOCIAL_CIRCLE_Binned\n",
      "0-5     244544\n",
      "6-10     22184\n",
      "11+       3556\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DEF_30_CNT_SOCIAL_CIRCLE_Binned:\n",
      "DEF_30_CNT_SOCIAL_CIRCLE_Binned\n",
      "0-5     270222\n",
      "6-10        61\n",
      "11+          1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DEF_60_CNT_SOCIAL_CIRCLE_Binned:\n",
      "DEF_60_CNT_SOCIAL_CIRCLE_Binned\n",
      "0-5     270262\n",
      "6-10        21\n",
      "11+          1\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering\n",
    "df3['AGE'] = df3['DAYS_BIRTH'] // 365\n",
    "df3['YEARS_EMPLOYED'] = df3['DAYS_EMPLOYED'] // 365\n",
    "df3['YEARS_REGISTRATION'] = df3['DAYS_REGISTRATION'] // 365\n",
    "df3['YEARS_ID_PUBLISH'] = df3['DAYS_ID_PUBLISH'] // 365\n",
    "df3['YEARS_LAST_PHONE_CHANGE'] =df3['DAYS_LAST_PHONE_CHANGE'] /365\n",
    "\n",
    "# Define bin edges and labels for AGE_Binned\n",
    "age_bin_edges = [18, 30, 40, 50, 60, 70, 80]\n",
    "age_bin_labels = ['18-30', '31-40', '41-50', '51-60', '61-70', '+71']\n",
    "\n",
    "# Define bin edges and labels for YEARS_EMPLOYED_Binned and YEARS_REGISTRATION_Binned\n",
    "employment_bin_edges = [0, 5, 12, 20, float('inf')]\n",
    "employment_bin_labels = ['0-5', '6-12', '13-20', '+25']\n",
    "\n",
    "# Apply binning to the newly created features\n",
    "df3['AGE_Binned'] = pd.cut(df3['AGE'], bins=age_bin_edges, labels=age_bin_labels, right=False)\n",
    "df3['YEARS_EMPLOYED_Binned'] = pd.cut(df3['YEARS_EMPLOYED'], bins=employment_bin_edges, labels=employment_bin_labels, right=False)\n",
    "df3['YEARS_REGISTRATION_Binned'] = pd.cut(df3['YEARS_REGISTRATION'], bins=employment_bin_edges, labels=employment_bin_labels, right=False)\n",
    "df3['YEARS_ID_PUBLISH_Binned'] = pd.cut(df3['YEARS_ID_PUBLISH'], bins=age_bin_edges, labels=age_bin_labels, right=False)\n",
    "\n",
    "# Drop original columns\n",
    "df3 = df3.drop(columns_to_remove_sign, axis=1)\n",
    "\n",
    "bin_edges = [0, 5, 10, float('inf')]\n",
    "bin_labels = ['0-5', '6-10', '11+']\n",
    "\n",
    "df3['OBS_30_CNT_SOCIAL_CIRCLE_Binned'] = pd.cut(df3['OBS_30_CNT_SOCIAL_CIRCLE'], bins=bin_edges, labels=bin_labels, right=False)\n",
    "df3['OBS_60_CNT_SOCIAL_CIRCLE_Binned'] = pd.cut(df3['OBS_60_CNT_SOCIAL_CIRCLE'], bins=bin_edges, labels=bin_labels, right=False)\n",
    "df3['DEF_30_CNT_SOCIAL_CIRCLE_Binned'] = pd.cut(df3['DEF_30_CNT_SOCIAL_CIRCLE'], bins=bin_edges, labels=bin_labels, right=False)\n",
    "df3['DEF_60_CNT_SOCIAL_CIRCLE_Binned'] = pd.cut(df3['DEF_60_CNT_SOCIAL_CIRCLE'], bins=bin_edges, labels=bin_labels, right=False)\n",
    "\n",
    "for col in ['OBS_30_CNT_SOCIAL_CIRCLE_Binned', 'OBS_60_CNT_SOCIAL_CIRCLE_Binned', \n",
    "            'DEF_30_CNT_SOCIAL_CIRCLE_Binned', 'DEF_60_CNT_SOCIAL_CIRCLE_Binned']:\n",
    "    bin_counts = df3[col].value_counts().sort_index()\n",
    "    print(f\"{col}:\\n{bin_counts}\\n\")\n",
    "\n",
    "df3 = df3.drop(['OBS_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', \n",
    "              'DEF_30_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23b2fafd-c992-4026-a1ce-72b00634a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['YEARS_ID_PUBLISH_Binned', 'YEARS_ID_PUBLISH', 'FLAG_MOBIL',\n",
    "                   'FLAG_EMP_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_EMAIL','REG_REGION_NOT_WORK_REGION',\n",
    "                   'REG_REGION_NOT_LIVE_REGION','REG_CITY_NOT_WORK_CITY','REG_CITY_NOT_LIVE_CITY']\n",
    "df3 = df3.drop(columns_to_drop,axis=1)\n",
    "df3 = df3[df3['CODE_GENDER'] != 'XNA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f230425-4c7d-43d7-8f53-7ab4da22811b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68384238-1daf-4099-80e8-5e0ab482c4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 270280 entries, 0 to 307510\n",
      "Data columns (total 47 columns):\n",
      " #   Column                             Non-Null Count   Dtype   \n",
      "---  ------                             --------------   -----   \n",
      " 0   SK_ID_CURR                         270280 non-null  object  \n",
      " 1   TARGET                             270280 non-null  int64   \n",
      " 2   NAME_CONTRACT_TYPE                 270280 non-null  object  \n",
      " 3   CODE_GENDER                        270280 non-null  object  \n",
      " 4   FLAG_OWN_CAR                       270280 non-null  object  \n",
      " 5   FLAG_OWN_REALTY                    270280 non-null  object  \n",
      " 6   CNT_CHILDREN                       270280 non-null  int64   \n",
      " 7   AMT_INCOME_TOTAL                   270280 non-null  float64 \n",
      " 8   AMT_CREDIT                         270280 non-null  float64 \n",
      " 9   AMT_ANNUITY                        270280 non-null  float64 \n",
      " 10  AMT_GOODS_PRICE                    270280 non-null  float64 \n",
      " 11  NAME_TYPE_SUITE                    270280 non-null  object  \n",
      " 12  NAME_INCOME_TYPE                   270280 non-null  object  \n",
      " 13  NAME_EDUCATION_TYPE                270280 non-null  object  \n",
      " 14  NAME_FAMILY_STATUS                 270280 non-null  object  \n",
      " 15  NAME_HOUSING_TYPE                  270280 non-null  object  \n",
      " 16  REGION_POPULATION_RELATIVE         270280 non-null  float64 \n",
      " 17  FLAG_WORK_PHONE                    270280 non-null  category\n",
      " 18  FLAG_PHONE                         270280 non-null  category\n",
      " 19  OCCUPATION_TYPE                    270280 non-null  object  \n",
      " 20  CNT_FAM_MEMBERS                    270280 non-null  float64 \n",
      " 21  REGION_RATING_CLIENT               270280 non-null  category\n",
      " 22  REGION_RATING_CLIENT_W_CITY        270280 non-null  category\n",
      " 23  WEEKDAY_APPR_PROCESS_START         270280 non-null  object  \n",
      " 24  HOUR_APPR_PROCESS_START            270280 non-null  int64   \n",
      " 25  LIVE_REGION_NOT_WORK_REGION        270280 non-null  category\n",
      " 26  LIVE_CITY_NOT_WORK_CITY            270280 non-null  category\n",
      " 27  ORGANIZATION_TYPE                  270280 non-null  object  \n",
      " 28  EXT_SOURCE_2                       270280 non-null  float64 \n",
      " 29  NUM_DOCUMENTS_PROVIDED             270280 non-null  category\n",
      " 30  AMT_REQ_CREDIT_BUREAU_YEAR_Binned  270280 non-null  category\n",
      " 31  AMT_REQ_CREDIT_BUREAU_QRT_Binned   270280 non-null  category\n",
      " 32  AMT_REQ_CREDIT_BUREAU_MON_Binned   270280 non-null  category\n",
      " 33  AMT_REQ_CREDIT_BUREAU_DAY_Binned   270280 non-null  category\n",
      " 34  AMT_REQ_CREDIT_BUREAU_WEEK_Binned  270280 non-null  category\n",
      " 35  AMT_REQ_CREDIT_BUREAU_HOUR_Binned  270280 non-null  category\n",
      " 36  AGE                                270280 non-null  int64   \n",
      " 37  YEARS_EMPLOYED                     270280 non-null  int64   \n",
      " 38  YEARS_REGISTRATION                 270280 non-null  float64 \n",
      " 39  YEARS_LAST_PHONE_CHANGE            270280 non-null  float64 \n",
      " 40  AGE_Binned                         270280 non-null  category\n",
      " 41  YEARS_EMPLOYED_Binned              270280 non-null  category\n",
      " 42  YEARS_REGISTRATION_Binned          270280 non-null  category\n",
      " 43  OBS_30_CNT_SOCIAL_CIRCLE_Binned    270280 non-null  category\n",
      " 44  OBS_60_CNT_SOCIAL_CIRCLE_Binned    270280 non-null  category\n",
      " 45  DEF_30_CNT_SOCIAL_CIRCLE_Binned    270280 non-null  category\n",
      " 46  DEF_60_CNT_SOCIAL_CIRCLE_Binned    270280 non-null  category\n",
      "dtypes: category(20), float64(9), int64(5), object(13)\n",
      "memory usage: 62.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb0cef0-ece8-4915-803f-9ecca4f2a2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc8c7071-5b70-411f-8708-1eca23a8fed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 270280 entries, 0 to 307510\n",
      "Data columns (total 14 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   TARGET                      270280 non-null  int64  \n",
      " 1   CNT_CHILDREN                270280 non-null  int64  \n",
      " 2   AMT_INCOME_TOTAL            270280 non-null  float64\n",
      " 3   AMT_CREDIT                  270280 non-null  float64\n",
      " 4   AMT_ANNUITY                 270280 non-null  float64\n",
      " 5   AMT_GOODS_PRICE             270280 non-null  float64\n",
      " 6   REGION_POPULATION_RELATIVE  270280 non-null  float64\n",
      " 7   CNT_FAM_MEMBERS             270280 non-null  float64\n",
      " 8   HOUR_APPR_PROCESS_START     270280 non-null  int64  \n",
      " 9   EXT_SOURCE_2                270280 non-null  float64\n",
      " 10  AGE                         270280 non-null  int64  \n",
      " 11  YEARS_EMPLOYED              270280 non-null  int64  \n",
      " 12  YEARS_REGISTRATION          270280 non-null  float64\n",
      " 13  YEARS_LAST_PHONE_CHANGE     270280 non-null  float64\n",
      "dtypes: float64(9), int64(5)\n",
      "memory usage: 30.9 MB\n",
      "base nova:  None\n"
     ]
    }
   ],
   "source": [
    "# Selecionar colunas numéricas (int64 e float64)\n",
    "col_numericas = df3.select_dtypes(include=['int64', 'float64']).columns\n",
    "numericas = []\n",
    "\n",
    "for coluna in col_numericas:\n",
    "    numericas.append(coluna)\n",
    "\n",
    "df_arvore_3 = df3[numericas]\n",
    "print('base nova: ', df_arvore_3.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efbf9692-76b2-48b1-a3c8-5915fa8aaf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.8296\n",
      "Precisão: 0.1364\n",
      "Recall: 0.1649\n",
      "F1-Score: 0.1493\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[44039  5116]\n",
      " [ 4093   808]]\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91     49155\n",
      "           1       0.14      0.16      0.15      4901\n",
      "\n",
      "    accuracy                           0.83     54056\n",
      "   macro avg       0.53      0.53      0.53     54056\n",
      "weighted avg       0.84      0.83      0.84     54056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definir as features e o target\n",
    "X = df_arvore_3.drop(columns=['TARGET'])  # Todas as variáveis menos o target\n",
    "y = df_arvore_3['TARGET']  # O target\n",
    "\n",
    "# Dividir os dados em treino e teste (80% treino, 20% teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar o modelo de árvore de decisão\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões nos dados de teste\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# Avaliar o desempenho do modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f'Acurácia: {accuracy:.4f}')\n",
    "print(f'Precisão: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')\n",
    "print('\\nMatriz de Confusão:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\nRelatório de Classificação:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cba25656-4c26-46ba-b361-e27085b7c0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Melhores parâmetros: {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Acurácia: 0.6397\n",
      "Precisão: 0.1496\n",
      "Recall: 0.6346\n",
      "F1-Score: 0.2421\n"
     ]
    }
   ],
   "source": [
    "# Treinamento da árvore com class_weight balanceado\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parâmetros a serem ajustados\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Modelo de árvore\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Grid Search com validação cruzada\n",
    "grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=5, scoring='f1', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Treinamento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Avaliação\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"Melhores parâmetros: {grid_search.best_params_}\")\n",
    "print(f'Acurácia: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print(f'Precisão: {precision_score(y_test, y_pred):.4f}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred):.4f}')\n",
    "print(f'F1-Score: {f1_score(y_test, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883954a4-f91a-475e-8078-e572ae1bbe9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ecb5a91-9c57-4a1c-8ecf-f7326fe4d4d1",
   "metadata": {},
   "source": [
    "Ao analisar os resultados dos dois cenários:\n",
    "\n",
    "### 1. **Árvore original**:\n",
    "- **Acurácia**: 0.8296\n",
    "- **Precisão**: 0.1364\n",
    "- **Recall**: 0.1649\n",
    "- **F1-Score**: 0.1493\n",
    "- **Matriz de Confusão**:\n",
    "  - Verdadeiros Negativos (classe 0): 44,039\n",
    "  - Falsos Positivos: 5,116\n",
    "  - Falsos Negativos: 4,093\n",
    "  - Verdadeiros Positivos (classe 1): 808\n",
    "\n",
    "Neste cenário, a acurácia é boa (82.96%), mas a **precisão** e o **recall** para a classe de fraudes (classe 1) ainda são baixos, indicando que o modelo está identificando poucas fraudes corretamente. O F1-Score de 0.1493 mostra que o modelo tem dificuldade em equilibrar a precisão e o recall para a detecção de fraudes.\n",
    "\n",
    "### 2. **Árvore com `class_weight='balanced'`**:\n",
    "- **Acurácia**: 0.6397\n",
    "- **Precisão**: 0.1496\n",
    "- **Recall**: 0.6346\n",
    "- **F1-Score**: 0.2421\n",
    "\n",
    "Após aplicar o ajuste de `class_weight='balanced'`, a acurácia diminuiu significativamente para 63.97%. No entanto, houve uma grande melhora no **recall** (63.46%), o que significa que o modelo está conseguindo identificar a maior parte das fraudes. A **precisão** ainda é relativamente baixa (14.96%), o que indica a presença de falsos positivos. No entanto, o F1-Score melhorou para 0.2421, o que mostra que o modelo está conseguindo um melhor equilíbrio entre precisão e recall.\n",
    "\n",
    "### **Conclusão**:\n",
    "O modelo com `class_weight='balanced'` é mais adequado para a detecção de fraudes. Mesmo com a queda na acurácia, o aumento significativo no recall é importante, pois ele reflete a capacidade do modelo de identificar fraudes. Embora ainda haja um número considerável de falsos positivos, o aumento no recall é geralmente mais desejável em um cenário de detecção de fraudes, onde é preferível detectar mais fraudes, mesmo que com algum custo de precisão. O F1-Score mais alto (0.2421) também sugere que o desempenho geral na identificação de fraudes melhorou com a abordagem balanceada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3764d1fd-37e4-4fc5-8018-318b7a295ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6f68c-51ed-4da8-a1fa-ef2a7ac9cb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e859dab6-5fdc-448c-b2e9-7c59c706d7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e7cc6-c353-41d4-9fa1-10c1d6472f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c8caf-90e8-4eb1-84cf-79d03013d989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
